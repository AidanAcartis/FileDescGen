Tr√®s bonne question üëç tu touches pile √† la diff√©rence entre :

* **Fine-tune classique int√©gral** ‚Üí on entra√Æne *tous* les param√®tres du mod√®le (comme ton code avec `T5WithFusion`).
* **PEFT/LoRA** ‚Üí on **g√®le le mod√®le de base** et on n‚Äôentra√Æne que de petites couches d‚Äôadaptation (*LoRA adapters*).

---

## ‚ö° Dans ton cas (avec `T5WithFusion`)

Comme tu as un mod√®le **custom** (T5WithFusion qui ajoute les `lexical_embeds` au premier token), tu dois combiner **les deux √©tapes** :

1. Cr√©er ton mod√®le custom (`T5WithFusion`).
2. Appliquer PEFT/LoRA *par-dessus*.

---

### üöÄ √âtapes concr√®tes

#### 1. D√©finir ton mod√®le fusionn√©

```python
model = T5WithFusion("google/flan-t5-small")
```

#### 2. D√©finir la config LoRA

Ici, on cible les modules internes de T5 (souvent `q`, `v`, ou `k` dans les `attention` layers).

```python
from peft import LoraConfig, get_peft_model, TaskType

lora_config = LoraConfig(
    r=32,
    lora_alpha=32,
    target_modules=["q", "v"],  # tu peux aussi tester ["k", "o"] selon l‚Äôexp√©rience
    lora_dropout=0.05,
    bias="none",
    task_type=TaskType.SEQ_2_SEQ_LM
)
```

#### 3. Appliquer LoRA √† ton mod√®le fusionn√©

```python
peft_model = get_peft_model(model, lora_config)
```

‚ö†Ô∏è Ici, `model` est ton **T5WithFusion**, pas le T5 de base.

#### 4. V√©rifier les param√®tres entra√Ænables

```python
from peft import get_peft_model_state_dict

def print_trainable_parameters(model):
    trainable = 0
    total = 0
    for _, param in model.named_parameters():
        total += param.numel()
        if param.requires_grad:
            trainable += param.numel()
    print(f"Trainable params: {trainable} / {total} ({100 * trainable/total:.2f}%)")

print_trainable_parameters(peft_model)
```

‚Üí tu devrais voir seulement **quelques millions** de param√®tres entra√Ænables (vs. 77M pour T5-small entier).

#### 5. D√©finir Trainer avec PEFT

```python
from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir="./results_peft",
    learning_rate=1e-3,  # plus haut qu‚Äôen full fine-tune
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    logging_steps=10,
    save_strategy="epoch",
    evaluation_strategy="epoch",
)

trainer = Trainer(
    model=peft_model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,  # ton DataCollatorWithFusion
)

trainer.train()
```

#### 6. Sauvegarde

```python
peft_model.save_pretrained("./peft_t5_withfusion")
tokenizer.save_pretrained("./peft_t5_withfusion")
```

---

## üîë R√©sum√©

* **Full fine-tune (ton premier code)** : tout le mod√®le est ajust√©, co√ªteux en VRAM et temps.
* **LoRA fine-tune (PEFT)** : seul un petit sous-ensemble de param√®tres est entra√Æn√©, beaucoup plus l√©ger.
* Dans ton cas ‚Üí tu **wrap ton T5WithFusion avec LoRA** (au lieu de T5 direct).

---

üëâ Veux-tu que je t‚Äô√©crive une **version inference adapt√©e pour ton mod√®le T5WithFusion + LoRA** (comment faire `generate()` avec les embeddings lexicaux) ?
